{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6634119a-9ea4-46da-bb3c-478a0775758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97ad7147-2f7e-41d1-b71b-2a403b85491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "def get_model_tokenizer(src_lang=\"en\", tgt_lang=\"hi\"):\n",
    "    model_name = f\"Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}\"\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c0d4837-5179-48b5-b06a-e93b5249d329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text, tokenizer, model):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    translation = model.generate(**tokens)\n",
    "    translated_text = tokenizer.decode(translation[0], skip_special_tokens=True)\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19359537-8a27-4bd0-b5ce-011f5826fdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hxtreme\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Please submit your claim form within 30 days from the date of incident.\n",
      "Hindi: ‡§á‡§∏ ‡§ò‡§ü‡§®‡§æ ‡§ï‡•Ä ‡§§‡§æ‡§∞‡•Ä‡§ñ ‡§∏‡•á 30 ‡§¶‡§ø‡§® ‡§ï‡•á ‡§Ö‡§Ç‡§¶‡§∞ ‡§Ö‡§™‡§®‡§æ ‡§¶‡§æ‡§µ‡§æ ‡§ú‡§º‡§æ‡§π‡§ø‡§∞ ‡§ï‡•Ä‡§ú‡§ø‡§è ‡•§\n"
     ]
    }
   ],
   "source": [
    "# Load translator: English ‚ûú Hindi\n",
    "tokenizer, model = get_model_tokenizer(\"en\", \"hi\")\n",
    "\n",
    "#en‚Üíhi (Hindi)\n",
    "#en‚Üíta (Tamil)\n",
    "#en‚Üífr, en‚Üíde, en‚Üíes (European)\n",
    "#en‚Üízh (Chinese)\n",
    "\n",
    "# Sample insurance sentence\n",
    "text = \"Please submit your claim form within 30 days from the date of incident.\"\n",
    "translated = translate_text(text, tokenizer, model)\n",
    "\n",
    "print(\"Original:\", text)\n",
    "print(\"Hindi:\", translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7af795c1-4b01-49ab-8aa1-845622f0ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers sentencepiece python-docx PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "803df0ff-4fc2-4008-b119-ee2676ca3784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import docx\n",
    "from docx import Document\n",
    "from transformers import MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "257741e8-0ff5-4109-81a1-e8bf15cbed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Load Translation Model ----------\n",
    "def load_translation_model(src_lang=\"en\", tgt_lang=\"hi\"):\n",
    "    model_name = f\"Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}\"\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5f357dc-89a3-40c5-bade-26470328a827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Translate Text ----------\n",
    "def translate_text(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    translated = model.generate(**inputs)\n",
    "    return tokenizer.decode(translated[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4a3e1cb-991a-42b6-a55b-d7771c8bef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Extract Text ----------\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    return \"\\n\".join([page.get_text() for page in doc])\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = docx.Document(docx_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13dce455-777a-4f1f-a88d-2841e4a8914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Save to New PDF ----------\n",
    "def save_to_pdf(text, output_path):\n",
    "    doc = fitz.open()\n",
    "    page = doc.new_page()\n",
    "    text_lines = text.split(\"\\n\")\n",
    "    y = 72\n",
    "    for line in text_lines:\n",
    "        page.insert_text((72, y), line, fontsize=11)\n",
    "        y += 14\n",
    "    doc.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2db22a0-2211-4ac3-b3cf-559b16345162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Save to New DOCX ----------\n",
    "def save_to_docx(text, output_path):\n",
    "    doc = Document()\n",
    "    for para in text.split(\"\\n\"):\n",
    "        doc.add_paragraph(para)\n",
    "    doc.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2dfc524-714e-45e1-9f56-698b66e522d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_and_save(file_path, file_type, target_lang=\"hi\"):\n",
    "    # Step 1: Define allowed Hugging Face target languages\n",
    "    supported_langs = [\"hi\", \"fr\", \"es\", \"de\", \"zh\"]  # Hindi, French, Spanish, German, Chinese\n",
    "\n",
    "    if target_lang not in supported_langs:\n",
    "        raise ValueError(\n",
    "            f\"‚ùå '{target_lang}' is not currently supported.\\n\"\n",
    "            f\"‚úÖ Try one of: {', '.join(supported_langs)}\"\n",
    "        )\n",
    "\n",
    "    print(f\"üîÑ Translating to: {target_lang.upper()} using HuggingFace model...\")\n",
    "\n",
    "    #Step 2: Load the model/tokenizer\n",
    "    tokenizer, model = load_translation_model(\"en\", target_lang)\n",
    "\n",
    "    #Step 3: Extract text from file\n",
    "    if file_type == \"pdf\":\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif file_type == \"docx\":\n",
    "        text = extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"‚ùå Unsupported file type! Use 'pdf' or 'docx'.\")\n",
    "\n",
    "    #Step 4: Translate line-by-line\n",
    "    lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
    "    translated = [translate_text(line, tokenizer, model) for line in lines]\n",
    "    full_translated_text = \"\\n\".join(translated)\n",
    "\n",
    "    #Step 5: Save to translated output\n",
    "    output_path = file_path.replace(\".pdf\", f\"_{target_lang}.pdf\") if file_type == \"pdf\" else file_path.replace(\".docx\", f\"_{target_lang}.docx\")\n",
    "\n",
    "    if file_type == \"pdf\":\n",
    "        save_to_pdf(full_translated_text, output_path)\n",
    "    else:\n",
    "        save_to_docx(full_translated_text, output_path)\n",
    "\n",
    "    print(f\"‚úÖ Translated content saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c5f194f-9c8d-4bbc-883e-5b4591f529ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Translating to: FR using HuggingFace model...\n",
      "‚úÖ Translated content saved to: C:\\Users\\Hxtreme\\Jupyter_Notebook_Learning\\Final_Project\\Dataset\\Aspire_Policy_Wordings_fr.pdf\n"
     ]
    }
   ],
   "source": [
    "# File to translate\n",
    "file_path = r\"C:\\Users\\Hxtreme\\Jupyter_Notebook_Learning\\Final_Project\\Dataset\\Aspire_Policy_Wordings.pdf\"\n",
    "file_type = \"pdf\"\n",
    "\n",
    "#Language selection logic\n",
    "language_options = {\n",
    "    \"Hindi\": \"hi\",\n",
    "    \"French\": \"fr\",\n",
    "    \"Spanish\": \"es\",\n",
    "    \"German\": \"de\",\n",
    "    \"Chinese\": \"zh\"\n",
    "}\n",
    "selected_language = \"French\"  # Change here\n",
    "target_lang = language_options[selected_language]\n",
    "\n",
    "# Translate and save\n",
    "translate_and_save(file_path, file_type, target_lang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebfc6255-da4e-4372-b7ac-a49f714b973b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
      "     ---------------------------------------- 0.0/42.8 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/42.8 kB ? eta -:--:--\n",
      "     ------------------ ------------------- 20.5/42.8 kB 217.9 kB/s eta 0:00:01\n",
      "     -------------------------------------- 42.8/42.8 kB 260.1 kB/s eta 0:00:00\n",
      "Collecting pdfminer.six==20250327 (from pdfplumber)\n",
      "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfplumber) (10.2.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "     ---------------------------------------- 0.0/48.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 48.2/48.2 kB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfminer.six==20250327->pdfplumber) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfminer.six==20250327->pdfplumber) (42.0.2)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.21)\n",
      "Downloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 0.0/60.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 60.2/60.2 kB ? eta 0:00:00\n",
      "Downloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.6 MB 5.2 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.6/5.6 MB 7.4 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.0/5.6 MB 7.9 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.8/5.6 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.4/5.6 MB 10.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.0/5.6 MB 11.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.5/5.6 MB 11.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.1/5.6 MB 12.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.2/5.6 MB 11.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.4/5.6 MB 12.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.6/5.6 MB 12.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 11.6 MB/s eta 0:00:00\n",
      "Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.6/3.0 MB 12.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.1/3.0 MB 11.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.8/3.0 MB 12.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.3/3.0 MB 12.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.0/3.0 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 11.1 MB/s eta 0:00:00\n",
      "Installing collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
      "Successfully installed pdfminer.six-20250327 pdfplumber-0.11.6 pypdfium2-4.30.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script pypdfium2.exe is installed in 'C:\\Users\\Hxtreme\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script pdfplumber.exe is installed in 'C:\\Users\\Hxtreme\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "#!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afd89c7e-b74f-46b3-a62b-7aeca8b16675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting googletrans==4.0.0-rc1\n",
      "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.12.14)\n",
      "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\n",
      "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
      "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "   ---------------------------------------- 0.0/55.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 55.1/55.1 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 133.4/133.4 kB 4.0 MB/s eta 0:00:00\n",
      "Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "   ---------------------------------------- 0.0/42.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 42.6/42.6 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.8/58.8 kB ? eta 0:00:00\n",
      "Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "   ---------------------------------------- 0.0/65.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 65.0/65.0 kB 3.7 MB/s eta 0:00:00\n",
      "Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.3 MB 14.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.0/1.3 MB 12.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 11.9 MB/s eta 0:00:00\n",
      "Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "   ---------------------------------------- 0.0/53.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 53.6/53.6 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: googletrans\n",
      "  Building wheel for googletrans (setup.py): started\n",
      "  Building wheel for googletrans (setup.py): finished with status 'done'\n",
      "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17458 sha256=1e133429d5da304a86632c05e2e2cdf27f8a4c309d59945282a9971bbbbf6222\n",
      "  Stored in directory: c:\\users\\hxtreme\\appdata\\local\\pip\\cache\\wheels\\39\\17\\6f\\66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
      "Successfully built googletrans\n",
      "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.7\n",
      "    Uninstalling httpcore-1.0.7:\n",
      "      Successfully uninstalled httpcore-1.0.7\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.28.1\n",
      "    Uninstalling httpx-0.28.1:\n",
      "      Successfully uninstalled httpx-0.28.1\n",
      "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script chardetect.exe is installed in 'C:\\Users\\Hxtreme\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.75 requires requests_mock, which is not installed.\n",
      "jupyterlab 4.3.6 requires httpx>=0.25.0, but you have httpx 0.13.3 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
      "notebook 7.0.8 requires jupyterlab<4.1,>=4.0.2, but you have jupyterlab 4.3.6 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "#!pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a50ecf0-19b8-46e6-a463-add7f2193553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\hxtreme\\appdata\\roaming\\python\\python311\\site-packages (0.11.6)\n",
      "Requirement already satisfied: python-docx in c:\\users\\hxtreme\\appdata\\roaming\\python\\python311\\site-packages (1.1.2)\n",
      "Collecting deep-translator\n",
      "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: pdfminer.six==20250327 in c:\\users\\hxtreme\\appdata\\roaming\\python\\python311\\site-packages (from pdfplumber) (20250327)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfplumber) (10.2.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\hxtreme\\appdata\\roaming\\python\\python311\\site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfminer.six==20250327->pdfplumber) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfminer.six==20250327->pdfplumber) (42.0.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-docx) (4.9.3)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\hxtreme\\appdata\\roaming\\python\\python311\\site-packages (from python-docx) (4.13.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from deep-translator) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in c:\\users\\hxtreme\\appdata\\roaming\\python\\python311\\site-packages (from deep-translator) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hxtreme\\appdata\\roaming\\python\\python311\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2024.12.14)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.21)\n",
      "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
      "   ---------------------------------------- 0.0/42.3 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 10.2/42.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 42.3/42.3 kB 681.8 kB/s eta 0:00:00\n",
      "Installing collected packages: deep-translator\n",
      "Successfully installed deep-translator-1.11.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts deep-translator.exe and dt.exe are installed in 'C:\\Users\\Hxtreme\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber python-docx deep-translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d336cb54-cce9-4a64-870d-320aac596f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\hxtreme\\appdata\\roaming\\python\\python311\\site-packages (0.11.6)\n",
      "Requirement already satisfied: python-docx in c:\\users\\hxtreme\\appdata\\roaming\\python\\python311\\site-packages (1.1.2)\n",
      "Requirement already satisfied: deep-translator in c:\\users\\hxtreme\\appdata\\roaming\\python\\python311\\site-packages (1.11.4)\n",
      "Requirement already satisfied: pdfminer.six==20250327 in c:\\users\\hxtreme\\appdata\\roaming\\python\\python311\\site-packages (from pdfplumber) (20250327)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfplumber) (10.2.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\hxtreme\\appdata\\roaming\\python\\python311\\site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfminer.six==20250327->pdfplumber) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfminer.six==20250327->pdfplumber) (42.0.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-docx) (4.9.3)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\hxtreme\\appdata\\roaming\\python\\python311\\site-packages (from python-docx) (4.13.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from deep-translator) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in c:\\users\\hxtreme\\appdata\\roaming\\python\\python311\\site-packages (from deep-translator) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hxtreme\\appdata\\roaming\\python\\python311\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2024.12.14)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber python-docx deep-translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b4e4bf2-518b-4cc1-b918-702d4e4a119e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Translating from en ‚ûú hi\n",
      "\n",
      "‚úÖ Translated file saved to: C:\\Users\\Hxtreme\\Jupyter_Notebook_Learning\\Final_Project\\Dataset\\Aspire_Policy_Wordings_hi_v2.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import docx\n",
    "import time\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# --------- Extract Text ---------\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        return \"\\n\".join(page.extract_text() for page in pdf.pages if page.extract_text())\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = docx.Document(docx_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n",
    "\n",
    "# --------- Safe Translation ---------\n",
    "def translate_line_safe(line, src='en', dest='hi', retries=3, delay=1):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            translator = GoogleTranslator(source=src, target=dest, timeout=5)\n",
    "            return translator.translate(line)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Retry {attempt+1} failed for line: {line[:30]}... ‚ûú {e}\")\n",
    "            time.sleep(delay)\n",
    "    return line  # fallback to original\n",
    "\n",
    "# --------- Translate Full File ---------\n",
    "def translate_file(input_path, output_path, src_lang='en', dest_lang='hi'):\n",
    "    ext = os.path.splitext(input_path)[1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        text = extract_text_from_pdf(input_path)\n",
    "    elif ext == \".docx\":\n",
    "        text = extract_text_from_docx(input_path)\n",
    "    else:\n",
    "        raise ValueError(\"‚ùå Unsupported file format. Use PDF or DOCX.\")\n",
    "\n",
    "    print(f\"üîÑ Translating from {src_lang} ‚ûú {dest_lang}\")\n",
    "    \n",
    "    lines = [line.strip() for line in text.split(\"\\n\") if line.strip()]\n",
    "    translated_lines = [translate_line_safe(line, src_lang, dest_lang) for line in lines]\n",
    "    translated_text = \"\\n\\n\".join(translated_lines)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(translated_text)\n",
    "\n",
    "    print(f\"\\n‚úÖ Translated file saved to: {output_path}\")\n",
    "\n",
    "# --------- Example Usage ---------\n",
    "translate_file(\n",
    "    input_path=r\"C:\\Users\\Hxtreme\\Jupyter_Notebook_Learning\\Final_Project\\Dataset\\Aspire_Policy_Wordings.pdf\",\n",
    "    output_path=r\"C:\\Users\\Hxtreme\\Jupyter_Notebook_Learning\\Final_Project\\Dataset\\Aspire_Policy_Wordings_hi_v2.txt\",\n",
    "    src_lang=\"en\",\n",
    "    dest_lang=\"hi\"  # you can also use \"ta\", \"fr\", \"de\", etc.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f33e16-476f-462a-aef4-ea48378ef4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
